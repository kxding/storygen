{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation  \n",
    "[![Paper page](https://huggingface.co/datasets/huggingface/badges/resolve/main/paper-page-md-dark.svg)]()\n",
    "[[Paper]()] &emsp; [[Project Page]()] &emsp; <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/37/ahhfdkx/miniconda3/envs/storydiffusion/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "import requests\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import diffusers\n",
    "from diffusers import DDIMScheduler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils_sd15.gradio_utils import cal_attn_mask\n",
    "import copy\n",
    "import os\n",
    "from diffusers.utils import load_image\n",
    "from utils_sd15.utils import get_comic\n",
    "from utils_sd15.style_template import styles\n",
    "\n",
    "from animatediff_25.utils.util import save_videos_grid\n",
    "\n",
    "from animatediff_25.models.unet import UNet3DConditionModel\n",
    "from animatediff_25.pipelines.pipeline_animation import AnimationPipeline\n",
    "\n",
    "\n",
    "# from diffusers.models.attention_processor import AttentionProcessor, AttnProcessor, LoRAAttnProcessor\n",
    "\n",
    "from utils_sd15.gradio_utils import \\\n",
    "    AttnProcessor2_0 as AttnProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global\n",
    "STYLE_NAMES = list(styles.keys())\n",
    "DEFAULT_STYLE_NAME = \"(No style)\"\n",
    "MAX_SEED = np.iinfo(np.int32).max\n",
    "global models_dict\n",
    "use_va = False\n",
    "models_dict = {\n",
    "   \"Juggernaut\":\"RunDiffusion/Juggernaut-XL-v8\",\n",
    "   \"RealVision\":\"SG161222/RealVisXL_V4.0\" ,\n",
    "   \"SDXL\":\"stabilityai/stable-diffusion-xl-base-1.0\" ,\n",
    "   \"Unstable\": \"stablediffusionapi/sdxl-unstable-diffusers-y\",\n",
    "   \"SD15\": \"runwayml/stable-diffusion-v1-5\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    \n",
    "#################################################\n",
    "########Consistent Self-Attention################\n",
    "#################################################\n",
    "class SpatialAttnProcessor2_0(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Attention processor for IP-Adapater for PyTorch 2.0.\n",
    "    Args:\n",
    "        hidden_size (`int`):\n",
    "            The hidden size of the attention layer.\n",
    "        cross_attention_dim (`int`):\n",
    "            The number of channels in the `encoder_hidden_states`.\n",
    "        text_context_len (`int`, defaults to 77):\n",
    "            The context length of the text features.\n",
    "        scale (`float`, defaults to 1.0):\n",
    "            the weight scale of image prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size = None, cross_attention_dim=None,id_length = 4,device = \"cuda\",dtype = torch.float16, single_model_length = 16):\n",
    "        super().__init__()\n",
    "        if not hasattr(F, \"scaled_dot_product_attention\"):\n",
    "            raise ImportError(\"AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.\")\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cross_attention_dim = cross_attention_dim\n",
    "        self.total_length = id_length + 1\n",
    "        self.id_length = id_length\n",
    "        self.id_bank = {}\n",
    "        self.single_model_length = single_model_length\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        attention_mask=None,\n",
    "        temb=None):\n",
    "        global total_count,attn_count,cur_step,mask256,mask1024,mask4096\n",
    "        global sa32, sa64\n",
    "        global write\n",
    "        global height,width\n",
    "        \n",
    "        hidden_states = hidden_states.view(-1, self.single_model_length, hidden_states.shape[-2], hidden_states.shape[-1])\n",
    "        # print(\"temb = \", temb)\n",
    "        # print(\"cur_step = \", cur_step)\n",
    "        \n",
    "        if write:\n",
    "            # print(f\"white:{cur_step}\")\n",
    "            self.id_bank[cur_step] = [hidden_states[:self.id_length], hidden_states[self.id_length:]]\n",
    "        else:\n",
    "            \n",
    "            encoder_hidden_states = torch.cat((self.id_bank[cur_step][0].to(self.device),hidden_states[:1],self.id_bank[cur_step][1].to(self.device),hidden_states[1:]))\n",
    " \n",
    "        # skip in early step\n",
    "        if cur_step <5:\n",
    "            hidden_states = self.__call2__(attn, hidden_states,encoder_hidden_states,attention_mask,temb)\n",
    "        else:   # 256 1024 4096\n",
    "            random_number = random.random()\n",
    "            if cur_step <20:\n",
    "                rand_num = 0.3\n",
    "            else:\n",
    "                rand_num = 0.1\n",
    "            # if random_number > rand_num:\n",
    "            # if cur_step > 11: ### ??? \n",
    "            if False:\n",
    "                if write == False:\n",
    "                    if hidden_states.shape[2] == 256 :\n",
    "                        attention_mask = mask256[mask256.shape[0] // self.total_length * self.id_length:]\n",
    "                        # print(\"hidden_states.shape[1] == 256, attention_mask\", attention_mask.shape)\n",
    "                    elif hidden_states.shape[2] == 1024:\n",
    "                        attention_mask = mask1024[mask1024.shape[0] // self.total_length * self.id_length:]\n",
    "                        # print(\"hidden_states.shape[1] == 1024, attention_mask\", attention_mask.shape)\n",
    "                    elif hidden_states.shape[2] == 4096:\n",
    "                        attention_mask = mask4096[mask4096.shape[0] // self.total_length * self.id_length:]\n",
    "                    else:\n",
    "                        print(\"hidden_states.shape[1], attention_mask = \", hidden_states.shape)\n",
    "                else:\n",
    "                    if hidden_states.shape[2] == 256:\n",
    "                        attention_mask = mask256[:mask256.shape[0] // self.total_length * self.id_length,:mask256.shape[0] // self.total_length * self.id_length]\n",
    "                        # print(\"hidden_states.shape[1], attention_mask = \", hidden_states.shape, attention_mask.shape)\n",
    "                    elif hidden_states.shape[2] == 1024:\n",
    "                        attention_mask = mask1024[:mask1024.shape[0] // self.total_length * self.id_length,:mask1024.shape[0] // self.total_length * self.id_length]\n",
    "                        # print(\"hidden_states.shape[1], attention_mask\", hidden_states.shape ,attention_mask.shape)\n",
    "                    elif hidden_states.shape[2] == 4096:\n",
    "                        attention_mask = mask4096[:mask4096.shape[0] // self.total_length * self.id_length,:mask4096.shape[0] // self.total_length * self.id_length]\n",
    "                        # print(\"hidden_states.shape[1], attention_mask = \", hidden_states.shape,attention_mask.shape)\n",
    "                    else:\n",
    "                        print(\"hidden_states.shape[1], attention_mask = \", hidden_states.shape)\n",
    "                        \n",
    "                hidden_states = self.__call1__(attn, hidden_states, encoder_hidden_states, attention_mask, temb)\n",
    "            else:\n",
    "                hidden_states = self.__call2__(attn, hidden_states, None, attention_mask, temb)\n",
    "        attn_count +=1\n",
    "        if attn_count == total_count:\n",
    "            attn_count = 0\n",
    "            cur_step += 1\n",
    "            mask256,mask1024,mask4096 = cal_attn_mask(total_length,id_length,sa16, sa32,sa64,device=self.device, dtype= self.dtype)\n",
    "        \n",
    "        return hidden_states\n",
    "    def __call1__(\n",
    "        self,\n",
    "        attn,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        attention_mask=None,\n",
    "        temb=None,\n",
    "    ):\n",
    "        # __import__(\"ipdb\").set_trace()\n",
    "        if encoder_hidden_states is not None:\n",
    "            print(\"encoder_hidden_states.shape = \", encoder_hidden_states.shape)\n",
    "        # from animatediff_25.models.unet import timesteps\n",
    "        # print(\"timesteps = \", timesteps)\n",
    "        \n",
    "        # __import__('ipdb').set_trace()\n",
    "        residual = hidden_states\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 5:\n",
    "            batch_size, video_length, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(batch_size, video_length, channel, height * width).transpose(2, 3)\n",
    "            \n",
    "        batch_size, video_length, nums_token, channel = hidden_states.shape\n",
    "        \n",
    "        hidden_states_frame1 = hidden_states[:, 0, :, :]\n",
    "        hidden_states_frame1 = hidden_states_frame1.view(-1, nums_token, channel)\n",
    "        img_nums = batch_size//2\n",
    "        hidden_states_frame1 = hidden_states_frame1.view(-1,img_nums,nums_token,channel).reshape(-1,img_nums * nums_token,channel)\n",
    "        # print(\"hidden_states_frame1.shape = \", hidden_states_frame1.shape)\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states_frame1 = attn.group_norm(hidden_states_frame1.transpose(1, 2)).transpose(1, 2)\n",
    "        \n",
    "        # hidden_states = hidden_states.view(-1, nums_token, channel) \n",
    "        # hidden_states = hidden_states[:, 0, :, :]\n",
    "        # print(\"new hidden_states.shape = \", hidden_states.shape)\n",
    "        # hidden_states = hidden_states.view(-1, nums_token, channel)\n",
    "\n",
    "        query = attn.to_q(hidden_states_frame1)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states_frame1  # B, N, C\n",
    "        else:\n",
    "            encoder_hidden_states = encoder_hidden_states.view(-1,self.id_length+1,nums_token,channel).reshape(-1,(self.id_length+1) * nums_token,channel)\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "\n",
    "\n",
    "        inner_dim = key.shape[-1]\n",
    "        head_dim = inner_dim // attn.heads\n",
    "\n",
    "        batch_size_frame1 = hidden_states_frame1.shape[0]\n",
    "        query = query.view(batch_size_frame1, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        key = key.view(batch_size_frame1, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size_frame1, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        # print(\"query.shape, key.shape, value.shape = \", query.shape, key.shape, value.shape)\n",
    "        hidden_states_frame1 = F.scaled_dot_product_attention(\n",
    "            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n",
    "        )\n",
    "        print(\"hidden_states_frame1.shape\", hidden_states_frame1.shape)\n",
    "        print(\"hidden_states.shape\", hidden_states.shape)\n",
    "        hidden_states[:, 0, :, :] = hidden_states_frame1.view(-1, nums_token, channel)\n",
    "        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size * video_length , -1, attn.heads * head_dim)\n",
    "        hidden_states = hidden_states.to(query.dtype)\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "        print(\"final hidden_states.shape = \", hidden_states.shape)\n",
    "\n",
    "        if input_ndim == 5:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, video_length, channel, height, width)\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "        # print(hidden_states.shape)\n",
    "        return hidden_states\n",
    "    \n",
    "    def __call2__(\n",
    "        self,\n",
    "        attn,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        attention_mask=None,\n",
    "        temb=None):\n",
    "        residual = hidden_states\n",
    "\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 5:\n",
    "            batch_size, video_length, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(batch_size, video_length, channel, height * width).transpose(2, 3)\n",
    "\n",
    "\n",
    "        batch_size, video_length, sequence_length, channel = (\n",
    "            hidden_states.shape\n",
    "        )\n",
    "        # print(hidden_states.shape)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attn.prepare_attention_mask(attention_mask, video_length, sequence_length, batch_size)\n",
    "            # scaled_dot_product_attention expects attention_mask shape to be\n",
    "            # (batch, heads, source_length, target_length)\n",
    "            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        hidden_states = hidden_states.view(-1, sequence_length, channel)\n",
    "        query = attn.to_q(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states  # B, N, C\n",
    "        else:\n",
    "            encoder_hidden_states = encoder_hidden_states.view(-1,self.id_length+1,sequence_length,channel).reshape(-1,(self.id_length+1) * sequence_length,channel)\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "\n",
    "        inner_dim = key.shape[-1]\n",
    "        head_dim = inner_dim // attn.heads\n",
    "\n",
    "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        hidden_states = F.scaled_dot_product_attention(\n",
    "            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n",
    "        )\n",
    "\n",
    "        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size * video_length, -1, attn.heads * head_dim)\n",
    "        hidden_states = hidden_states.to(query.dtype)\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "        \n",
    "        if input_ndim == 5:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, video_length, channel, height, width)\n",
    "\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_length 3\n",
      "total_length 4\n",
      "single_model_length 16\n",
      "height, width 256 256\n",
      "num_steps 8\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "config = OmegaConf.load(\"./config/inference.yaml\")\n",
    "global attn_count, total_count, id_length, total_length,cur_step, cur_model_type\n",
    "global write\n",
    "global sa16, sa32, sa64\n",
    "global height,width\n",
    "\n",
    "global attn_procs,unet\n",
    "\n",
    "global pipeline\n",
    "global sd_model_path\n",
    "global num_steps\n",
    "attn_procs = {}\n",
    "\n",
    "attn_count = 0\n",
    "total_count = 0\n",
    "cur_step = 0\n",
    "id_length = 3\n",
    "cur_model_type = \"\"\n",
    "device=\"cuda\"\n",
    "###\n",
    "write = False\n",
    "### strength of consistent self-attention: the larger, the stronger\n",
    "sa16 = 0.5\n",
    "sa32 = 0.5\n",
    "sa64 = 0.5\n",
    "### Res. of the Generated Comics. Please Note: SDXL models may do worse in a low-resolution! \n",
    "height = config.get(\"height\", 256)\n",
    "width = config.get(\"width\", 256)\n",
    "with_csa = config.get(\"with_csa\", False)\n",
    "single_model_length = config.get(\"single_model_length\", 4)\n",
    "num_steps = config.get(\"steps\", 20)\n",
    "total_length = id_length + 1\n",
    "print(\"id_length\", id_length)\n",
    "print(\"total_length\", total_length)\n",
    "print(\"single_model_length\", single_model_length)\n",
    "print(\"height, width\", height, width)\n",
    "print(\"num_steps\", num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded temporal unet's pretrained weights from ../models/stable-diffusion-v1-5/unet ...\n",
      "self.attn_processors dict_keys([])\n",
      "### missing keys: 672; \n",
      "### unexpected keys: 0;\n",
      "### Temporal Module Parameters: 417.1376 M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from diffusers import AutoencoderKL, EulerDiscreteScheduler\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPTextModelWithProjection\n",
    "\n",
    "sd_model_path = \"../models/stable-diffusion-v1-5\"\n",
    "# Load Component\n",
    "tokenizer    = CLIPTokenizer.from_pretrained(sd_model_path, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(sd_model_path, subfolder=\"text_encoder\")\n",
    "vae          = AutoencoderKL.from_pretrained(sd_model_path, subfolder=\"vae\")   \n",
    "\n",
    "# init unet model\n",
    "unet = UNet3DConditionModel.from_pretrained_2d(sd_model_path, subfolder=\"unet\", unet_additional_kwargs=OmegaConf.to_container(config.unet_additional_kwargs))\n",
    "\n",
    "# print(\"unet.state_dict()\", unet.state_dict().keys())\n",
    "\n",
    "scheduler=DDIMScheduler(**OmegaConf.to_container(config.noise_scheduler_kwargs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert CSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successsfully load consistent self-attention\n",
      "number of the processor : 9\n",
      "using consistent self-attention\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Insert PairedAttention\n",
    "for name in unet.attn_processors.keys():\n",
    "    cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "    if name.startswith(\"mid_block\"):\n",
    "        hidden_size = unet.config.block_out_channels[-1]\n",
    "        # print(\"hidden_size\", hidden_size)\n",
    "    elif name.startswith(\"up_blocks\"):\n",
    "        block_id = int(name[len(\"up_blocks.\")])\n",
    "        hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "    elif name.startswith(\"down_blocks\"):\n",
    "        block_id = int(name[len(\"down_blocks.\")])\n",
    "        hidden_size = unet.config.block_out_channels[block_id]\n",
    "    if cross_attention_dim is None and (name.startswith(\"up_blocks\") ) :\n",
    "        # print(\"id_length\", id_length)\n",
    "        attn_procs[name] =  SpatialAttnProcessor2_0(id_length = id_length, single_model_length=single_model_length)\n",
    "        total_count +=1\n",
    "    else:\n",
    "        attn_procs[name] = AttnProcessor()\n",
    "    # print(\"hidden_size\", hidden_size)\n",
    "print(\"successsfully load consistent self-attention\")\n",
    "print(f\"number of the processor : {total_count}\")\n",
    "\n",
    "if with_csa:\n",
    "    unet.set_attn_processor(copy.deepcopy(attn_procs))\n",
    "    print(\"using consistent self-attention\")\n",
    "# print(\"unet\", unet )\n",
    "    \n",
    "mask256,mask1024,mask4096 = cal_attn_mask(total_length,id_length,sa16, sa32,sa64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get AnimationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load motion module from ../AnimateDiff/models/Motion_Module/mm_sd_v15.ckpt\n"
     ]
    }
   ],
   "source": [
    "from animatediff_25.utils.util import load_weights\n",
    "pipeline = AnimationPipeline(\n",
    "    vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, unet=unet,\n",
    "    scheduler=scheduler,\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "pipeline = load_weights(\n",
    "            pipeline,\n",
    "            # motion module\n",
    "            motion_module_path         = \"../AnimateDiff/models/Motion_Module/mm_sd_v15.ckpt\",\n",
    "        ).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load motion module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the text description for the comics\n",
    "Tips: Existing text2image diffusion models may not always generate images that accurately match text descriptions. Our training-free approach can improve the consistency of characters, but it does not enhance the control over the text. Therefore, in some cases, you may need to carefully craft your prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_scale = 5.0\n",
    "seed = 2047\n",
    "sa32 = 0.5\n",
    "sa64 = 0.5\n",
    "general_prompt = \"a girl with a blue hair\"\n",
    "negative_prompt = \"naked, deformed, bad anatomy, disfigured, poorly drawn face, mutation, extra limb, ugly, disgusting, poorly drawn hands, missing limb, floating limbs, disconnected limbs, blurry, watermarks, oversaturated, distorted hands, amputation\"\n",
    "prompt_array = [\n",
    "                \"have breakfast\",\n",
    "                \"is on the road, go to the company\",\n",
    "                \"work in the company\",\n",
    "                \"running in the playground\",\n",
    "                \"reading book in the home\"\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompts =  ['a girl with a blue hair,have breakfast', 'a girl with a blue hair,is on the road, go to the company', 'a girl with a blue hair,work in the company', 'a girl with a blue hair,running in the playground', 'a girl with a blue hair,reading book in the home']\n",
      "id_prompts =  ['a girl with a blue hair,have breakfast', 'a girl with a blue hair,is on the road, go to the company', 'a girl with a blue hair,work in the company']\n",
      "real_prompts =  ['a girl with a blue hair,running in the playground', 'a girl with a blue hair,reading book in the home']\n",
      "negative_prompt =  naked, deformed, bad anatomy, disfigured, poorly drawn face, mutation, extra limb, ugly, disgusting, poorly drawn hands, missing limb, floating limbs, disconnected limbs, blurry, watermarks, oversaturated, distorted hands, amputation\n",
      "negative_prompt =  photograph, deformed, glitch, noisy, realistic, stock photo, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry n\n"
     ]
    }
   ],
   "source": [
    "def apply_style_positive(style_name: str, positive: str):\n",
    "    p, n = styles.get(style_name, styles[DEFAULT_STYLE_NAME])\n",
    "    return p.replace(\"{prompt}\", positive) \n",
    "def apply_style(style_name: str, positives: list, negative: str = \"\"):\n",
    "    p, n = styles.get(style_name, styles[DEFAULT_STYLE_NAME])\n",
    "    return [p.replace(\"{prompt}\", positive) for positive in positives], n + ' ' + negative\n",
    "### Set the generated Style\n",
    "style_name = \"Comic book\"\n",
    "# setup_seed(seed)\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "prompts = [general_prompt+\",\"+prompt for prompt in prompt_array]\n",
    "id_prompts = prompts[:id_length]\n",
    "real_prompts = prompts[id_length:]\n",
    "print(\"prompts = \", prompts)\n",
    "print(\"id_prompts = \", id_prompts)\n",
    "print(\"real_prompts = \", real_prompts)\n",
    "print(\"negative_prompt = \", negative_prompt)\n",
    "negative_prompt = negative_prompt[0]\n",
    "torch.cuda.empty_cache()\n",
    "cur_step = 0\n",
    "attn_count = 0\n",
    "savedir = \"./output\"\n",
    "id_prompts, negative_prompt = apply_style(style_name, id_prompts, negative_prompt)\n",
    "\n",
    "# negative_prompt = negative_prompt  * len(id_prompts)\n",
    "# negative_prompt = negative_prompt[0]  * len(id_prompts)\n",
    "\n",
    "print(\"negative_prompt = \", negative_prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/37/ahhfdkx/storygen_video_sd15/animatediff_25/pipelines/pipeline_animation.py:368: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet3DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet3DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n",
      "  num_channels_latents = self.unet.in_channels\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps  tensor(876, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:02<00:20,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps  tensor(751, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [00:05<00:16,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps  tensor(626, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [00:08<00:13,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps  tensor(501, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [00:10<00:10,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps  tensor(376, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [00:13<00:07,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps  tensor(251, device='cuda:0')\n",
      "hidden_states_frame1.shape torch.Size([2, 8, 768, 80])\n",
      "hidden_states.shape torch.Size([6, 16, 256, 640])\n",
      "final hidden_states.shape =  torch.Size([96, 256, 640])\n",
      "hidden_states_frame1.shape torch.Size([2, 8, 768, 80])\n",
      "hidden_states.shape torch.Size([6, 16, 256, 640])\n",
      "final hidden_states.shape =  torch.Size([96, 256, 640])\n",
      "hidden_states_frame1.shape torch.Size([2, 8, 3072, 40])\n",
      "hidden_states.shape torch.Size([6, 16, 1024, 320])\n",
      "final hidden_states.shape =  torch.Size([96, 1024, 320])\n",
      "hidden_states_frame1.shape torch.Size([2, 8, 3072, 40])\n",
      "hidden_states.shape torch.Size([6, 16, 1024, 320])\n",
      "final hidden_states.shape =  torch.Size([96, 1024, 320])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [00:15<00:04,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps  tensor(126, device='cuda:0')\n",
      "hidden_states.shape[1], attention_mask =  torch.Size([6, 16, 64, 1280])\n",
      "hidden_states_frame1.shape torch.Size([2, 8, 192, 160])\n",
      "hidden_states.shape torch.Size([6, 16, 64, 1280])\n",
      "final hidden_states.shape =  torch.Size([96, 64, 1280])\n",
      "hidden_states.shape[1], attention_mask =  torch.Size([6, 16, 64, 1280])\n",
      "hidden_states_frame1.shape torch.Size([2, 8, 192, 160])\n",
      "hidden_states.shape torch.Size([6, 16, 64, 1280])\n",
      "final hidden_states.shape =  torch.Size([96, 64, 1280])\n",
      "hidden_states_frame1.shape torch.Size([2, 8, 768, 80])\n",
      "hidden_states.shape torch.Size([6, 16, 256, 640])\n",
      "final hidden_states.shape =  torch.Size([96, 256, 640])\n",
      "hidden_states_frame1.shape torch.Size([2, 8, 768, 80])\n",
      "hidden_states.shape torch.Size([6, 16, 256, 640])\n",
      "final hidden_states.shape =  torch.Size([96, 256, 640])\n",
      "hidden_states_frame1.shape torch.Size([2, 8, 768, 80])\n",
      "hidden_states.shape torch.Size([6, 16, 256, 640])\n",
      "final hidden_states.shape =  torch.Size([96, 256, 640])\n",
      "hidden_states_frame1.shape torch.Size([2, 8, 3072, 40])\n",
      "hidden_states.shape torch.Size([6, 16, 1024, 320])\n",
      "final hidden_states.shape =  torch.Size([96, 1024, 320])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [00:17<00:02,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps  tensor(1, device='cuda:0')\n",
      "hidden_states.shape[1], attention_mask =  torch.Size([6, 16, 64, 1280])\n",
      "hidden_states_frame1.shape torch.Size([2, 8, 192, 160])\n",
      "hidden_states.shape torch.Size([6, 16, 64, 1280])\n",
      "final hidden_states.shape =  torch.Size([96, 64, 1280])\n",
      "hidden_states.shape[1], attention_mask =  torch.Size([6, 16, 64, 1280])\n",
      "hidden_states_frame1.shape torch.Size([2, 8, 192, 160])\n",
      "hidden_states.shape torch.Size([6, 16, 64, 1280])\n",
      "final hidden_states.shape =  torch.Size([96, 64, 1280])\n",
      "hidden_states_frame1.shape torch.Size([2, 8, 768, 80])\n",
      "hidden_states.shape torch.Size([6, 16, 256, 640])\n",
      "final hidden_states.shape =  torch.Size([96, 256, 640])\n",
      "hidden_states_frame1.shape torch.Size([2, 8, 768, 80])\n",
      "hidden_states.shape torch.Size([6, 16, 256, 640])\n",
      "final hidden_states.shape =  torch.Size([96, 256, 640])\n",
      "hidden_states_frame1.shape torch.Size([2, 8, 768, 80])\n",
      "hidden_states.shape torch.Size([6, 16, 256, 640])\n",
      "final hidden_states.shape =  torch.Size([96, 256, 640])\n",
      "hidden_states_frame1.shape torch.Size([2, 8, 3072, 40])\n",
      "hidden_states.shape torch.Size([6, 16, 1024, 320])\n",
      "final hidden_states.shape =  torch.Size([96, 1024, 320])\n",
      "hidden_states_frame1.shape torch.Size([2, 8, 3072, 40])\n",
      "hidden_states.shape torch.Size([6, 16, 1024, 320])\n",
      "final hidden_states.shape =  torch.Size([96, 1024, 320])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:19<00:00,  2.39s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.00 GiB (GPU 0; 23.69 GiB total capacity; 16.20 GiB already allocated; 2.13 GiB free; 21.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m write \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m----> 3\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideo_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msingle_model_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m\t\t\t\t  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m\t\t\t  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvideos\n",
      "File \u001b[0;32m~/miniconda3/envs/storydiffusion/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/storygen_video_sd15/animatediff_25/pipelines/pipeline_animation.py:419\u001b[0m, in \u001b[0;36mAnimationPipeline.__call__\u001b[0;34m(self, prompt, video_length, height, width, num_inference_steps, guidance_scale, negative_prompt, num_videos_per_prompt, eta, generator, latents, output_type, return_dict, callback, callback_steps, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 callback(i, t, latents)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# Post-processing\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m video \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_latents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m# Convert to tensor\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/storygen_video_sd15/animatediff_25/pipelines/pipeline_animation.py:246\u001b[0m, in \u001b[0;36mAnimationPipeline.decode_latents\u001b[0;34m(self, latents)\u001b[0m\n\u001b[1;32m    244\u001b[0m latents \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m0.18215\u001b[39m \u001b[38;5;241m*\u001b[39m latents\n\u001b[1;32m    245\u001b[0m latents \u001b[38;5;241m=\u001b[39m rearrange(latents, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb c f h w -> (b f) c h w\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 246\u001b[0m video \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m    247\u001b[0m video \u001b[38;5;241m=\u001b[39m rearrange(video, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(b f) c h w -> b c f h w\u001b[39m\u001b[38;5;124m\"\u001b[39m, f\u001b[38;5;241m=\u001b[39mvideo_length)\n\u001b[1;32m    248\u001b[0m video \u001b[38;5;241m=\u001b[39m (video \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/storydiffusion/lib/python3.10/site-packages/diffusers/utils/accelerate_utils.py:46\u001b[0m, in \u001b[0;36mapply_forward_hook.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hf_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpre_forward(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/storydiffusion/lib/python3.10/site-packages/diffusers/models/autoencoders/autoencoder_kl.py:304\u001b[0m, in \u001b[0;36mAutoencoderKL.decode\u001b[0;34m(self, z, return_dict, generator)\u001b[0m\n\u001b[1;32m    302\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(decoded_slices)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (decoded,)\n",
      "File \u001b[0;32m~/miniconda3/envs/storydiffusion/lib/python3.10/site-packages/diffusers/models/autoencoders/autoencoder_kl.py:275\u001b[0m, in \u001b[0;36mAutoencoderKL._decode\u001b[0;34m(self, z, return_dict)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtiled_decode(z, return_dict\u001b[38;5;241m=\u001b[39mreturn_dict)\n\u001b[1;32m    274\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_quant_conv(z)\n\u001b[0;32m--> 275\u001b[0m dec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (dec,)\n",
      "File \u001b[0;32m~/miniconda3/envs/storydiffusion/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/storydiffusion/lib/python3.10/site-packages/diffusers/models/autoencoders/vae.py:338\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, sample, latent_embeds)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;66;03m# up\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m up_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_blocks:\n\u001b[0;32m--> 338\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[43mup_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# post-process\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m latent_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/storydiffusion/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/storydiffusion/lib/python3.10/site-packages/diffusers/models/unet_2d_blocks.py:2535\u001b[0m, in \u001b[0;36mUpDecoderBlock2D.forward\u001b[0;34m(self, hidden_states, temb, scale)\u001b[0m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2534\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m upsampler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers:\n\u001b[0;32m-> 2535\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mupsampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2537\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/storydiffusion/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/storydiffusion/lib/python3.10/site-packages/diffusers/models/upsampling.py:184\u001b[0m, in \u001b[0;36mUpsample2D.forward\u001b[0;34m(self, hidden_states, output_size, scale)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv, LoRACompatibleConv) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m USE_PEFT_BACKEND:\n\u001b[0;32m--> 184\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/storydiffusion/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/storydiffusion/lib/python3.10/site-packages/diffusers/models/lora.py:358\u001b[0m, in \u001b[0;36mLoRACompatibleConv.forward\u001b[0;34m(self, hidden_states, scale)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, scale: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m         \u001b[38;5;66;03m# make sure to the functional Conv2D function as otherwise torch.compile's graph will break\u001b[39;00m\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;66;03m# see: https://github.com/huggingface/diffusers/pull/4315\u001b[39;00m\n\u001b[0;32m--> 358\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m         original_outputs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    363\u001b[0m             hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[1;32m    364\u001b[0m         )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.00 GiB (GPU 0; 23.69 GiB total capacity; 16.20 GiB already allocated; 2.13 GiB free; 21.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "write = True\n",
    "with torch.inference_mode():\n",
    "    samples = pipeline(\n",
    "        prompt=id_prompts,\n",
    "        negative_prompt=negative_prompt,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=num_steps,\n",
    "        video_length = single_model_length,\n",
    "        width\t\t\t\t  = width,\n",
    "        height\t\t\t  = height,\n",
    "    ).videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample.shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43msamples\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      2\u001b[0m save_videos_grid(samples, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msavedir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/sample.gif\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave to\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msavedir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/sample.gif\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'samples' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"sample.shape\", samples.shape)\n",
    "save_videos_grid(samples, f\"{savedir}/sample.gif\", n_rows=4)\n",
    "print(\"save to\", f\"{savedir}/sample.gif\")\n",
    "i = 0\n",
    "for sample in samples:\n",
    "    print(\"sample.shape\", sample.shape)\n",
    "    print(\"i\", i)\n",
    "    id_prompt = id_prompts[i]\n",
    "    print(\"id_prompt\", id_prompt)\n",
    "    sample = sample.unsqueeze(0)\n",
    "    save_videos_grid(sample, f\"{savedir}/{id_prompt}.gif\")\n",
    "    i = i + 1\n",
    "print(f\"save to {savedir}/{id_prompt}.gif\")\n",
    "    \n",
    "save_videos_grid(samples, f\"{savedir}/sample.gif\", n_rows=4)\n",
    "\n",
    "OmegaConf.save(config, f\"{savedir}/config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continued Creation\n",
    "From now on, you can create endless stories about this character without worrying about memory constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make pictures into comics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "fileId": "51613593-0d85-430e-8fce-c85e580fc483",
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
