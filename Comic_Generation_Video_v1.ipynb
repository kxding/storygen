{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation  \n",
    "[![Paper page](https://huggingface.co/datasets/huggingface/badges/resolve/main/paper-page-md-dark.svg)]()\n",
    "[[Paper]()] &emsp; [[Project Page]()] &emsp; <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "import requests\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from utils.gradio_utils import is_torch2_available\n",
    "if is_torch2_available():\n",
    "    from utils.gradio_utils import \\\n",
    "        AttnProcessor2_0 as AttnProcessor\n",
    "else:\n",
    "    from utils.gradio_utils  import AttnProcessor\n",
    "\n",
    "import diffusers\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "from diffusers import DDIMScheduler\n",
    "import torch.nn.functional as F\n",
    "from utils.gradio_utils import cal_attn_mask_xl\n",
    "import copy\n",
    "import os\n",
    "from diffusers.utils import load_image\n",
    "from utils.utils import get_comic\n",
    "from utils.style_template import styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global\n",
    "STYLE_NAMES = list(styles.keys())\n",
    "DEFAULT_STYLE_NAME = \"(No style)\"\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    \n",
    "#################################################\n",
    "########Consistent Self-Attention################\n",
    "#################################################\n",
    "class SpatialAttnProcessor2_0(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Attention processor for IP-Adapater for PyTorch 2.0.\n",
    "    Args:\n",
    "        hidden_size (`int`):\n",
    "            The hidden size of the attention layer.\n",
    "        cross_attention_dim (`int`):\n",
    "            The number of channels in the `encoder_hidden_states`.\n",
    "        text_context_len (`int`, defaults to 77):\n",
    "            The context length of the text features.\n",
    "        scale (`float`, defaults to 1.0):\n",
    "            the weight scale of image prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size = None, cross_attention_dim=None,id_length = 4,device = \"cuda\",dtype = torch.float16):\n",
    "        super().__init__()\n",
    "        if not hasattr(F, \"scaled_dot_product_attention\"):\n",
    "            raise ImportError(\"AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.\")\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cross_attention_dim = cross_attention_dim\n",
    "        self.total_length = id_length + 1\n",
    "        self.id_length = id_length\n",
    "        self.id_bank = {}\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        attention_mask=None,\n",
    "        temb=None):\n",
    "        global total_count,attn_count,cur_step,mask1024,mask4096\n",
    "        global sa32, sa64\n",
    "        global write\n",
    "        global height,width\n",
    "        # __import__('ipdb').set_trace()\n",
    "        \n",
    "        if write:\n",
    "            # print(f\"white:{cur_step}\")\n",
    "            self.id_bank[cur_step] = [hidden_states[:self.id_length], hidden_states[self.id_length:]]\n",
    "        else:\n",
    "            encoder_hidden_states = torch.cat((self.id_bank[cur_step][0].to(self.device),hidden_states[:1],self.id_bank[cur_step][1].to(self.device),hidden_states[1:]))\n",
    "\n",
    "        # skip in early step\n",
    "        if cur_step <5:\n",
    "            hidden_states = self.__call2__(attn, hidden_states,encoder_hidden_states,attention_mask,temb)\n",
    "        else:   # 256 1024 4096\n",
    "            random_number = random.random()\n",
    "            if cur_step <20:\n",
    "                rand_num = 0.3\n",
    "            else:\n",
    "                rand_num = 0.1\n",
    "            if random_number > rand_num:\n",
    "                # __import__('ipdb').set_trace()\n",
    "            # if False:\n",
    "                if not write:\n",
    "                    if hidden_states.shape[1] == (height//32) * (width//32):\n",
    "                        attention_mask = mask1024[mask1024.shape[0] // self.total_length * self.id_length:]\n",
    "                    else:\n",
    "                        attention_mask = mask4096[mask4096.shape[0] // self.total_length * self.id_length:]\n",
    "                else:\n",
    "                    if hidden_states.shape[1] == (height//32) * (width//32):\n",
    "                        attention_mask = mask1024[:mask1024.shape[0] // self.total_length * self.id_length,:mask1024.shape[0] // self.total_length * self.id_length]\n",
    "                    else:\n",
    "                        attention_mask = mask4096[:mask4096.shape[0] // self.total_length * self.id_length,:mask4096.shape[0] // self.total_length * self.id_length]\n",
    "                if os.environ.get(\"DEBUG_MODE\") == \"true\":\n",
    "                    if encoder_hidden_states is not None:\n",
    "                        print(\"call encoder hidden_states: \", encoder_hidden_states.shape)\n",
    "                    else:\n",
    "                        print(\"call encoder hidden_states: None\")\n",
    "                    if hidden_states is not None:\n",
    "                        print(\"call hidden_states: \", hidden_states.shape)\n",
    "                    else:\n",
    "                        print(\"call hidden_states: None\")\n",
    "                    print(\"call attention_mask: \", attention_mask.shape)\n",
    "                hidden_states = self.__call1__(attn, hidden_states,encoder_hidden_states,attention_mask,temb)\n",
    "            else:\n",
    "                hidden_states = self.__call2__(attn, hidden_states,None,attention_mask,temb)\n",
    "        attn_count +=1\n",
    "        if attn_count == total_count:\n",
    "            attn_count = 0\n",
    "            cur_step += 1\n",
    "            print(\"height, width = \", height, width)\n",
    "            mask1024,mask4096 = cal_attn_mask_xl(self.total_length,self.id_length,sa32,sa64,height,width, device=self.device, dtype= self.dtype)\n",
    "            print(\"mask1024,mask4096 = \", mask1024.shape, mask4096.shape)\n",
    "            if attention_mask is not None:\n",
    "                print(\"attention_mask = \", attention_mask.shape)\n",
    "\n",
    "        return hidden_states\n",
    "    def __call1__(\n",
    "        self,\n",
    "        attn,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        attention_mask=None,\n",
    "        temb=None,\n",
    "    ):\n",
    "        if os.environ.get(\"DEBUG_MODE\") == \"true\":\n",
    "            print(\"call1 hidden_states: \", hidden_states.shape)\n",
    "        residual = hidden_states\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            total_batch_size, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(total_batch_size, channel, height * width).transpose(1, 2)\n",
    "        total_batch_size,nums_token,channel = hidden_states.shape\n",
    "        img_nums = total_batch_size//2\n",
    "        hidden_states = hidden_states.view(-1,img_nums,nums_token,channel).reshape(-1,img_nums * nums_token,channel)\n",
    "\n",
    "        batch_size, sequence_length, _ = hidden_states.shape\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        query = attn.to_q(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states  # B, N, C\n",
    "        else:\n",
    "            encoder_hidden_states = encoder_hidden_states.view(-1,self.id_length+1,nums_token,channel).reshape(-1,(self.id_length+1) * nums_token,channel)\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "\n",
    "\n",
    "        inner_dim = key.shape[-1]\n",
    "        head_dim = inner_dim // attn.heads\n",
    "\n",
    "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        \n",
    "        # print(\"call1 query, key, value, \", query.shape, key.shape, value.shape, )\n",
    "        # if attention_mask is not None:\n",
    "        #     print(\"attn_mask\", attention_mask.shape)\n",
    "        \n",
    "        # __import__('ipdb').set_trace()\n",
    "        \n",
    "        hidden_states = F.scaled_dot_product_attention(\n",
    "            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n",
    "        )\n",
    "\n",
    "        hidden_states = hidden_states.transpose(1, 2).reshape(total_batch_size, -1, attn.heads * head_dim)\n",
    "        hidden_states = hidden_states.to(query.dtype)\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(total_batch_size, channel, height, width)\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "        # print(hidden_states.shape)\n",
    "        return hidden_states\n",
    "   \n",
    "    def __call2__(\n",
    "        self,\n",
    "        attn,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        attention_mask=None,\n",
    "        temb=None):\n",
    "        \n",
    "        if os.environ.get(\"DEBUG_MODE\") == \"true\":\n",
    "            print(\"call2 hidden_states: \", hidden_states.shape)\n",
    "        residual = hidden_states\n",
    "\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            batch_size, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
    "\n",
    "        batch_size, sequence_length, channel = (\n",
    "            hidden_states.shape\n",
    "        )\n",
    "        # print(hidden_states.shape)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
    "            # scaled_dot_product_attention expects attention_mask shape to be\n",
    "            # (batch, heads, source_length, target_length)\n",
    "            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        query = attn.to_q(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states  # B, N, C\n",
    "        else:\n",
    "            encoder_hidden_states = encoder_hidden_states.view(-1,self.id_length+1,sequence_length,channel).reshape(-1,(self.id_length+1) * sequence_length,channel)\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "\n",
    "        inner_dim = key.shape[-1]\n",
    "        head_dim = inner_dim // attn.heads\n",
    "\n",
    "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        # print(\"call2 query, key, value, \", query.shape, key.shape, value.shape,)\n",
    "        # if attention_mask is not None:\n",
    "        #     print(\"call2 attention_mask: \", attention_mask.shape)\n",
    "        hidden_states = F.scaled_dot_product_attention(\n",
    "            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n",
    "        )\n",
    "\n",
    "        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
    "        hidden_states = hidden_states.to(query.dtype)\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n",
    "\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global attn_count, total_count, id_length, total_length,cur_step, cur_model_type\n",
    "global write\n",
    "global  sa32, sa64\n",
    "global height,width\n",
    "\n",
    "global attn_procs,unet\n",
    "attn_procs = {}\n",
    "\n",
    "attn_count = 0\n",
    "total_count = 0\n",
    "cur_step = 0\n",
    "id_length = 4\n",
    "total_length = 5\n",
    "cur_model_type = \"\"\n",
    "device=\"cuda\"\n",
    "###\n",
    "write = False\n",
    "### strength of consistent self-attention: the larger, the stronger\n",
    "sa32 = 0.5\n",
    "sa64 = 0.5\n",
    "### Res. of the Generated Comics. Please Note: SDXL models may do worse in a low-resolution! \n",
    "height = 512\n",
    "width = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "config = OmegaConf.load(\"./config/inference.yaml\")\n",
    "\n",
    "\n",
    "from diffusers import AutoencoderKL, EulerDiscreteScheduler\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPTextModelWithProjection\n",
    "\n",
    "sd_model_path = \"../models/sd_xl\"\n",
    "# Load Component\n",
    "tokenizer\t = CLIPTokenizer.from_pretrained(sd_model_path, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(sd_model_path, subfolder=\"text_encoder\")\n",
    "vae\t\t\t = AutoencoderKL.from_pretrained(sd_model_path, subfolder=\"vae\")\n",
    "tokenizer_two = CLIPTokenizer.from_pretrained(sd_model_path, subfolder=\"tokenizer_2\")\n",
    "text_encoder_two = CLIPTextModelWithProjection.from_pretrained(sd_model_path, subfolder=\"text_encoder_2\")\n",
    "\n",
    "from animatediff.models.unet import UNet3DConditionModel\n",
    "from animatediff.pipelines.pipeline_animation import AnimationPipeline\n",
    "# init unet model\n",
    "unet = UNet3DConditionModel.from_pretrained_2d(sd_model_path, subfolder=\"unet\", unet_additional_kwargs=OmegaConf.to_container(config.unet_additional_kwargs))\n",
    "# print(\"unet.state_dict()\", unet.state_dict().keys())\n",
    "\n",
    "\n",
    "scheduler = EulerDiscreteScheduler(timestep_spacing='leading', steps_offset=1,\t**config.noise_scheduler_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###\n",
    "# sd_model_path = \"../models/sd_xl\"\n",
    "# ### LOAD Stable Diffusion Pipeline\n",
    "# pipe = StableDiffusionXLPipeline.from_pretrained(sd_model_path, torch_dtype=torch.float16, use_safetensors=False)\n",
    "# pipe = pipe.to(device)\n",
    "# # pipe.enable_freeu(s1=0.6, s2=0.4, b1=1.1, b2=1.2)\n",
    "\n",
    "# from animatediff.models.unet import UNet3DConditionModel\n",
    "# from omegaconf import OmegaConf\n",
    "# config = OmegaConf.load(\"./config/inference.yaml\")\n",
    "# from diffusers import AutoencoderKL, EulerDiscreteScheduler\n",
    "# scheduler = EulerDiscreteScheduler(timestep_spacing='leading', steps_offset=1,\t**config.noise_scheduler_kwargs)\n",
    "# # pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "# # pipe.scheduler.set_timesteps(50)\n",
    "\n",
    "# unet = UNet3DConditionModel.from_pretrained_2d(sd_model_path, subfolder=\"unet\", unet_additional_kwargs=OmegaConf.to_container(config.unet_additional_kwargs))\n",
    "# # print(\"unet.state_dict()\", unet.state_dict().keys())\n",
    "\n",
    "# from animatediff.pipelines.pipeline_animation import AnimationPipeline\n",
    "# pipe = AnimationPipeline(\n",
    "#     unet=unet, vae=pipe.vae, tokenizer=pipe.tokenizer, text_encoder=pipe.text_encoder, scheduler=scheduler,\n",
    "#     text_encoder_2=pipe.text_encoder_2, tokenizer_2=pipe.tokenizer_2,\n",
    "# ).to(device)\n",
    "# print(\"pipe\", pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert PairedAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Insert PairedAttention\n",
    "for name in unet.attn_processors.keys():\n",
    "    cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "    if name.startswith(\"mid_block\"):\n",
    "        hidden_size = unet.config.block_out_channels[-1]\n",
    "    elif name.startswith(\"up_blocks\"):\n",
    "        block_id = int(name[len(\"up_blocks.\")])\n",
    "        hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "    elif name.startswith(\"down_blocks\"):\n",
    "        block_id = int(name[len(\"down_blocks.\")])\n",
    "        hidden_size = unet.config.block_out_channels[block_id]\n",
    "    if cross_attention_dim is None and (name.startswith(\"up_blocks\") ) :\n",
    "        attn_procs[name] =  SpatialAttnProcessor2_0(id_length = id_length)\n",
    "        total_count +=1\n",
    "    else:\n",
    "        attn_procs[name] = AttnProcessor()\n",
    "print(\"successsfully load consistent self-attention\")\n",
    "print(f\"number of the processor : {total_count}\")\n",
    "\n",
    "unet.set_attn_processor(copy.deepcopy(attn_procs))\n",
    "global mask1024,mask4096\n",
    "mask1024, mask4096 = cal_attn_mask_xl(total_length,id_length,sa32,sa64,height,width,device=device,dtype= torch.float16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将插入CSA的unet 投入到 AnimationPipeline中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline = AnimationPipeline(\n",
    "        unet=unet, vae=vae, tokenizer=tokenizer, text_encoder=text_encoder, scheduler=scheduler,\n",
    "        text_encoder_2 = text_encoder_two, tokenizer_2=tokenizer_two\n",
    ").to(\"cuda\")\n",
    "# print(\"pipeline = \", pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### insert motion module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# motion_module_path = \"/userhome/37/ahhfdkx/AnimateDiff_sdxl/models/Motion_Module/mm_sdxl_v10_beta.ckpt\"\n",
    "# motion_module_ckpt = torch.load(motion_module_path, map_location=\"cpu\")\n",
    "# motion_module_state_dict = {}\n",
    "\n",
    "# print(\"motion_module = \", motion_module_ckpt.keys())\n",
    "# print(\"pipe.unet.state_dict()\", pipe.unet.state_dict().keys())\n",
    "# m_k = None\n",
    "# for k, v in motion_module_ckpt.items():\n",
    "#     if 'motion_module' in k and k in pipe.unet.state_dict().keys():\n",
    "#         motion_module_state_dict[k] = v\n",
    "#         m_k = k\n",
    "#         # print(\"pipeline.unet.state_dict()\", pipeline.unet.state_dict().keys())\n",
    "#     elif 'motion_module' in k and k not in pipe.unet.state_dict().keys():\n",
    "#         print(k)\n",
    "# pipe.unet.load_state_dict(motion_module_state_dict, strict=False)\n",
    "\n",
    "# del motion_module_ckpt\n",
    "# del motion_module_state_dict\n",
    "# print(f'Loading motion module from {motion_module_path}...')\n",
    "\n",
    "# pipe.unet = pipe.unet.half()\n",
    "# pipe.text_encoder = pipe.text_encoder.half()\n",
    "# pipe.text_encoder_2 = pipe.text_encoder_2.half()\n",
    "# pipe.enable_model_cpu_offload()\n",
    "# pipe.enable_vae_slicing()\n",
    "# # print(unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from animatediff.utils.util import load_weights, save_videos_grid\n",
    "savedir = \"./output\"\n",
    "pipeline = load_weights(\n",
    "\t\tpipeline = pipeline,\n",
    "\t\tmotion_module_path = config.get(\"motion_module_path\", \"\"),\n",
    "\t\tckpt_path = config.get(\"ckpt_path\", \"\"),\n",
    "\t\tlora_path = config.get(\"lora_path\", \"\"),\n",
    "\t\tlora_alpha = config.get(\"lora_alpha\", 0.8)\n",
    "\t)\n",
    "\n",
    "pipeline.unet = pipeline.unet.half()\n",
    "pipeline.text_encoder = pipeline.text_encoder.half()\n",
    "pipeline.text_encoder_2 = pipeline.text_encoder_2.half()\n",
    "pipeline.enable_model_cpu_offload()\n",
    "pipeline.enable_vae_slicing()\n",
    "\n",
    "prompts\t   = config.prompt\n",
    "n_prompts  = config.n_prompt\n",
    "\n",
    "random_seeds = config.get(\"seed\", [-1])\n",
    "random_seeds = [random_seeds] if isinstance(random_seeds, int) else list(random_seeds)\n",
    "random_seeds = random_seeds * len(prompts) if len(random_seeds) == 1 else random_seeds\n",
    "seeds = []\n",
    "samples = []\n",
    "\n",
    "write = True\n",
    "with torch.inference_mode():\n",
    "    for prompt_idx, (prompt, n_prompt, random_seed) in enumerate(zip(prompts, n_prompts, random_seeds)):\n",
    "        # manually set random seed for reproduction\n",
    "\n",
    "        # if random_seed != -1: torch.manual_seed(random_seed)\n",
    "        # else: torch.seed()\n",
    "        # seeds.append(torch.initial_seed())\n",
    "        print(f\"current seed: {torch.initial_seed()}\")\n",
    "        print(f\"sampling {prompt} ...\")\n",
    "        print(\"n_prompt\", n_prompt)\n",
    "        print(\"prompt\", prompt)\n",
    "        sample = pipeline(\n",
    "            prompt,\n",
    "            negative_prompt\t  = n_prompt,\n",
    "            num_inference_steps = config.get('steps', 20),\n",
    "            guidance_scale\t  = config.get('guidance_scale', 10),\n",
    "            width\t\t\t\t  = width,\n",
    "            height\t\t\t  = height,\n",
    "            single_model_length = 4,\n",
    "        ).videos\n",
    "        print(\"sample = \", sample)\n",
    "        samples.append(sample)\n",
    "        # save video\n",
    "        save_videos_grid(sample, f\"{savedir}/b.mp4\")\n",
    "        print(f\"save to {savedir}/sample/{prompt}.mp4\")\n",
    "\n",
    "samples = torch.concat(samples)\n",
    "save_videos_grid(samples, f\"{savedir}/sample-{datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')}.mp4\", n_rows=4)\n",
    "# config.seed = seeds\n",
    "OmegaConf.save(config, f\"{savedir}/config.yaml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get(\"DEBUG_MODE\") == \"true\":\n",
    "    print(\"mask1024\", mask1024.shape)\n",
    "    print(\"mask4096\", mask4096.shape)\n",
    "    print(mask1024)\n",
    "    print(mask1024.shape[0] // 5 * 4)\n",
    "    print(mask1024[196:].shape)\n",
    "    total_length = 5\n",
    "    id_length = 4\n",
    "    attention_mask = mask1024[:mask1024.shape[0] // total_length * id_length,:mask1024.shape[0] // total_length * id_length]\n",
    "    print(attention_mask.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the text description for the comics\n",
    "Tips: Existing text2image diffusion models may not always generate images that accurately match text descriptions. Our training-free approach can improve the consistency of characters, but it does not enhance the control over the text. Therefore, in some cases, you may need to carefully craft your prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_scale = 10.0\n",
    "seed = 2047\n",
    "sa32 = 0.5\n",
    "sa64 = 0.5\n",
    "id_length = 4\n",
    "num_steps = 30\n",
    "\n",
    "general_prompt = \"a man with a black suit\"\n",
    "negative_prompt = \"naked, deformed, bad anatomy, disfigured, poorly drawn face, mutation, extra limb, ugly, disgusting, poorly drawn hands, missing limb, floating limbs, disconnected limbs, blurry, watermarks, oversaturated, distorted hands, amputation\"\n",
    "prompt_array = [\"wake up in the bed\",\n",
    "                \"have breakfast\",\n",
    "                \"is on the road, go to the company\",\n",
    "                \"work in the company\",\n",
    "                \"running in the playground\",\n",
    "                \"reading book in the home\"\n",
    "                ]\n",
    "\n",
    "def apply_style_positive(style_name: str, positive: str):\n",
    "    p, n = styles.get(style_name, styles[DEFAULT_STYLE_NAME])\n",
    "    return p.replace(\"{prompt}\", positive) \n",
    "def apply_style(style_name: str, positives: list, negative: str = \"\"):\n",
    "    p, n = styles.get(style_name, styles[DEFAULT_STYLE_NAME])\n",
    "    return [p.replace(\"{prompt}\", positive) for positive in positives], n + ' ' + negative\n",
    "### Set the generated Style\n",
    "style_name = \"Comic book\"\n",
    "setup_seed(seed)\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "print(id_length)\n",
    "prompts = [general_prompt + \" \" + prompt for prompt in prompt_array]\n",
    "id_prompts = prompts[:id_length]\n",
    "print(\"id_prompts\", id_prompts)\n",
    "real_prompts = prompts[id_length:]\n",
    "write = False\n",
    "cur_step = 0\n",
    "attn_count = 0\n",
    "_, negative_prompt = apply_style(style_name, id_prompts, negative_prompt)\n",
    "# id_prompts, negative_prompt = apply_style(style_name, id_prompts, negative_prompt)\n",
    "print(\"id_prompts\", id_prompts)\n",
    "print(id_prompts[0])\n",
    "print(negative_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DEBUG_MODE\"] = \"false\"\n",
    "from animatediff.utils.util import save_videos_grid\n",
    "write = True\n",
    "id_images = []\n",
    "print(\"negative_prompt\", negative_prompt)\n",
    "with torch.inference_mode():\n",
    "    for id_prompt in id_prompts:\n",
    "        print(\"cur_step\", cur_step)\n",
    "        print(\"id_prompt\", id_prompt)\n",
    "        torch.seed()\n",
    "        sample = pipe(\n",
    "                    id_prompt,\n",
    "                    num_inference_steps = num_steps, \n",
    "                    guidance_scale=guidance_scale, \n",
    "                    height = height, # height is global once initialized all the sames ！！！ [todo]\n",
    "                    width = width,\n",
    "                    # negative_prompt = negative_prompt,\n",
    "                    negative_prompt = \"\",\n",
    "                    single_model_length = 16,\n",
    "                    generator = generator\n",
    "                ).videos\n",
    "        id_images.append(sample)\n",
    "        print(\"sample \", sample)\n",
    "        save_videos_grid(sample, f\"./output/sample_{cur_step}.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continued Creation\n",
    "From now on, you can create endless stories about this character without worrying about memory constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make pictures into comics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "fileId": "51613593-0d85-430e-8fce-c85e580fc483",
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
