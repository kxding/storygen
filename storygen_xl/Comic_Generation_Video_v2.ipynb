{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation  \n",
    "[![Paper page](https://huggingface.co/datasets/huggingface/badges/resolve/main/paper-page-md-dark.svg)]()\n",
    "[[Paper]()] &emsp; [[Project Page]()] &emsp; <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/37/ahhfdkx/miniconda3/envs/storydiffusion/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "import requests\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from utils.gradio_utils import is_torch2_available\n",
    "if is_torch2_available():\n",
    "    from utils.gradio_utils import \\\n",
    "        AttnProcessor2_0 as AttnProcessor\n",
    "else:\n",
    "    from utils.gradio_utils  import AttnProcessor\n",
    "\n",
    "import diffusers\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "from diffusers import DDIMScheduler\n",
    "import torch.nn.functional as F\n",
    "from utils.gradio_utils import cal_attn_mask_xl\n",
    "import copy\n",
    "import os\n",
    "from diffusers.utils import load_image\n",
    "from utils.utils import get_comic\n",
    "from utils.style_template import styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global\n",
    "STYLE_NAMES = list(styles.keys())\n",
    "DEFAULT_STYLE_NAME = \"(No style)\"\n",
    "MAX_SEED = np.iinfo(np.int32).max\n",
    "global models_dict\n",
    "use_va = False\n",
    "models_dict = {\n",
    "   \"Juggernaut\":\"RunDiffusion/Juggernaut-XL-v8\",\n",
    "   \"RealVision\":\"SG161222/RealVisXL_V4.0\" ,\n",
    "   \"SDXL\":\"stabilityai/stable-diffusion-xl-base-1.0\" ,\n",
    "   \"Unstable\": \"stablediffusionapi/sdxl-unstable-diffusers-y\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    \n",
    "#################################################\n",
    "########Consistent Self-Attention################\n",
    "#################################################\n",
    "class SpatialAttnProcessor2_0(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Attention processor for IP-Adapater for PyTorch 2.0.\n",
    "    Args:\n",
    "        hidden_size (`int`):\n",
    "            The hidden size of the attention layer.\n",
    "        cross_attention_dim (`int`):\n",
    "            The number of channels in the `encoder_hidden_states`.\n",
    "        text_context_len (`int`, defaults to 77):\n",
    "            The context length of the text features.\n",
    "        scale (`float`, defaults to 1.0):\n",
    "            the weight scale of image prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size = None, cross_attention_dim=None,id_length = 4,device = \"cuda\",dtype = torch.float16):\n",
    "        super().__init__()\n",
    "        if not hasattr(F, \"scaled_dot_product_attention\"):\n",
    "            raise ImportError(\"AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.\")\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cross_attention_dim = cross_attention_dim\n",
    "        self.total_length = id_length + 1\n",
    "        self.id_length = id_length\n",
    "        self.id_bank = {}\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        attention_mask=None,\n",
    "        temb=None):\n",
    "        global total_count,attn_count,cur_step,mask1024,mask4096\n",
    "        global sa32, sa64\n",
    "        global write\n",
    "        global height,width\n",
    "        if write:\n",
    "            # print(f\"white:{cur_step}\")\n",
    "            self.id_bank[cur_step] = [hidden_states[:self.id_length], hidden_states[self.id_length:]]\n",
    "        else:\n",
    "            encoder_hidden_states = torch.cat((self.id_bank[cur_step][0].to(self.device),hidden_states[:1],self.id_bank[cur_step][1].to(self.device),hidden_states[1:]))\n",
    "        # skip in early step\n",
    "        if cur_step <5:\n",
    "            hidden_states = self.__call2__(attn, hidden_states,encoder_hidden_states,attention_mask,temb)\n",
    "        else:   # 256 1024 4096\n",
    "            random_number = random.random()\n",
    "            if cur_step <20:\n",
    "                rand_num = 0.3\n",
    "            else:\n",
    "                rand_num = 0.1\n",
    "            if random_number > rand_num:\n",
    "                # __import__('ipdb').set_trace()\n",
    "            # if False:\n",
    "                if not write:\n",
    "                    if hidden_states.shape[1] == (height//32) * (width//32):\n",
    "                        attention_mask = mask1024[mask1024.shape[0] // self.total_length * self.id_length:]\n",
    "                    else:\n",
    "                        attention_mask = mask4096[mask4096.shape[0] // self.total_length * self.id_length:]\n",
    "                else:\n",
    "                    if hidden_states.shape[1] == (height//32) * (width//32):\n",
    "                        attention_mask = mask1024[:mask1024.shape[0] // self.total_length * self.id_length,:mask1024.shape[0] // self.total_length * self.id_length]\n",
    "                    else:\n",
    "                        attention_mask = mask4096[:mask4096.shape[0] // self.total_length * self.id_length,:mask4096.shape[0] // self.total_length * self.id_length]\n",
    "                if os.environ.get(\"DEBUG_MODE\") == \"true\":\n",
    "                    if encoder_hidden_states is not None:\n",
    "                        print(\"call encoder hidden_states: \", encoder_hidden_states.shape)\n",
    "                    else:\n",
    "                        print(\"call encoder hidden_states: None\")\n",
    "                    if hidden_states is not None:\n",
    "                        print(\"call hidden_states: \", hidden_states.shape)\n",
    "                    else:\n",
    "                        print(\"call hidden_states: None\")\n",
    "                    print(\"call attention_mask: \", attention_mask.shape)\n",
    "                hidden_states = self.__call1__(attn, hidden_states,encoder_hidden_states,attention_mask,temb)\n",
    "            else:\n",
    "                hidden_states = self.__call2__(attn, hidden_states,None,attention_mask,temb)\n",
    "        attn_count +=1\n",
    "        if attn_count == total_count:\n",
    "            attn_count = 0\n",
    "            cur_step += 1\n",
    "            mask1024,mask4096 = cal_attn_mask_xl(self.total_length,self.id_length,sa32,sa64,height,width, device=self.device, dtype= self.dtype)\n",
    "\n",
    "        return hidden_states\n",
    "    def __call1__(\n",
    "        self,\n",
    "        attn,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        attention_mask=None,\n",
    "        temb=None,\n",
    "    ):\n",
    "        if os.environ.get(\"DEBUG_MODE\") == \"true\":\n",
    "            print(\"call1 hidden_states: \", hidden_states.shape)\n",
    "        residual = hidden_states\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            total_batch_size, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(total_batch_size, channel, height * width).transpose(1, 2)\n",
    "        total_batch_size,nums_token,channel = hidden_states.shape\n",
    "        img_nums = total_batch_size//2\n",
    "        hidden_states = hidden_states.view(-1,img_nums,nums_token,channel).reshape(-1,img_nums * nums_token,channel)\n",
    "\n",
    "        batch_size, sequence_length, _ = hidden_states.shape\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        query = attn.to_q(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states  # B, N, C\n",
    "        else:\n",
    "            encoder_hidden_states = encoder_hidden_states.view(-1,self.id_length+1,nums_token,channel).reshape(-1,(self.id_length+1) * nums_token,channel)\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "\n",
    "\n",
    "        inner_dim = key.shape[-1]\n",
    "        head_dim = inner_dim // attn.heads\n",
    "\n",
    "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        \n",
    "        # print(\"call1 query, key, value, \", query.shape, key.shape, value.shape, )\n",
    "        # if attention_mask is not None:\n",
    "        #     print(\"attn_mask\", attention_mask.shape)\n",
    "        \n",
    "        # __import__('ipdb').set_trace()\n",
    "        \n",
    "        hidden_states = F.scaled_dot_product_attention(\n",
    "            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n",
    "        )\n",
    "\n",
    "        hidden_states = hidden_states.transpose(1, 2).reshape(total_batch_size, -1, attn.heads * head_dim)\n",
    "        hidden_states = hidden_states.to(query.dtype)\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(total_batch_size, channel, height, width)\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "        # print(hidden_states.shape)\n",
    "        return hidden_states\n",
    "   \n",
    "    def __call2__(\n",
    "        self,\n",
    "        attn,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        attention_mask=None,\n",
    "        temb=None):\n",
    "        \n",
    "        if os.environ.get(\"DEBUG_MODE\") == \"true\":\n",
    "            print(\"call2 hidden_states: \", hidden_states.shape)\n",
    "        residual = hidden_states\n",
    "\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            batch_size, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
    "\n",
    "        batch_size, sequence_length, channel = (\n",
    "            hidden_states.shape\n",
    "        )\n",
    "        # print(hidden_states.shape)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
    "            # scaled_dot_product_attention expects attention_mask shape to be\n",
    "            # (batch, heads, source_length, target_length)\n",
    "            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        query = attn.to_q(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states  # B, N, C\n",
    "        else:\n",
    "            encoder_hidden_states = encoder_hidden_states.view(-1,self.id_length+1,sequence_length,channel).reshape(-1,(self.id_length+1) * sequence_length,channel)\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "\n",
    "        inner_dim = key.shape[-1]\n",
    "        head_dim = inner_dim // attn.heads\n",
    "\n",
    "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        # print(\"call2 query, key, value, \", query.shape, key.shape, value.shape,)\n",
    "        # if attention_mask is not None:\n",
    "        #     print(\"call2 attention_mask: \", attention_mask.shape)\n",
    "        hidden_states = F.scaled_dot_product_attention(\n",
    "            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n",
    "        )\n",
    "\n",
    "        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
    "        hidden_states = hidden_states.to(query.dtype)\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n",
    "\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "def set_attention_processor(unet,id_length):\n",
    "    attn_procs = {}\n",
    "    for name in unet.attn_processors.keys():\n",
    "        cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "        if name.startswith(\"mid_block\"):\n",
    "            hidden_size = unet.config.block_out_channels[-1]\n",
    "        elif name.startswith(\"up_blocks\"):\n",
    "            block_id = int(name[len(\"up_blocks.\")])\n",
    "            hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "        elif name.startswith(\"down_blocks\"):\n",
    "            block_id = int(name[len(\"down_blocks.\")])\n",
    "            hidden_size = unet.config.block_out_channels[block_id]\n",
    "        if cross_attention_dim is None:\n",
    "            if name.startswith(\"up_blocks\") :\n",
    "                attn_procs[name] = SpatialAttnProcessor2_0(id_length = id_length)\n",
    "            else:    \n",
    "                attn_procs[name] = AttnProcessor()\n",
    "        else:\n",
    "            attn_procs[name] = AttnProcessor()\n",
    "\n",
    "    unet.set_attn_processor(attn_procs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:03<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded temporal unet's pretrained weights from ../models/sd_xl/unet ...\n",
      "### missing keys: 420; \n",
      "### unexpected keys: 0;\n",
      "### Temporal Module Parameters: 236.7792 M\n"
     ]
    }
   ],
   "source": [
    "global attn_count, total_count, id_length, total_length,cur_step, cur_model_type\n",
    "global write\n",
    "global  sa32, sa64\n",
    "global height,width\n",
    "attn_count = 0\n",
    "total_count = 0\n",
    "cur_step = 0\n",
    "id_length = 4\n",
    "total_length = 5\n",
    "cur_model_type = \"\"\n",
    "device=\"cuda\"\n",
    "global attn_procs,unet\n",
    "attn_procs = {}\n",
    "###\n",
    "write = False\n",
    "### strength of consistent self-attention: the larger, the stronger\n",
    "sa32 = 0.5\n",
    "sa64 = 0.5\n",
    "### Res. of the Generated Comics. Please Note: SDXL models may do worse in a low-resolution! \n",
    "height = 512\n",
    "width = 512\n",
    "###\n",
    "global pipe\n",
    "global sd_model_path\n",
    "sd_model_path = \"../models/sd_xl\"\n",
    "from omegaconf import OmegaConf\n",
    "config = OmegaConf.load(\"./config/inference.yaml\")\n",
    "### LOAD Stable Diffusion Pipeline\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(sd_model_path, torch_dtype=torch.float16, use_safetensors=False)\n",
    "pipe = pipe.to(device)\n",
    "pipe.enable_freeu(s1=0.6, s2=0.4, b1=1.1, b2=1.2)\n",
    "# pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "from diffusers import AutoencoderKL, EulerDiscreteScheduler\n",
    "pipe.scheduler = EulerDiscreteScheduler(timestep_spacing='leading', steps_offset=1,\t**config.noise_scheduler_kwargs)\n",
    "from animatediff.models.unet import UNet3DConditionModel\n",
    "\n",
    "unet = UNet3DConditionModel.from_pretrained_2d(sd_model_path, subfolder=\"unet\", unet_additional_kwargs=OmegaConf.to_container(config.unet_additional_kwargs))\n",
    "\n",
    "# ### Insert PairedAttention\n",
    "# for name in unet.attn_processors.keys():\n",
    "#     cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "#     if name.startswith(\"mid_block\"):\n",
    "#         hidden_size = unet.config.block_out_channels[-1]\n",
    "#     elif name.startswith(\"up_blocks\"):\n",
    "#         block_id = int(name[len(\"up_blocks.\")])\n",
    "#         hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "#     elif name.startswith(\"down_blocks\"):\n",
    "#         block_id = int(name[len(\"down_blocks.\")])\n",
    "#         hidden_size = unet.config.block_out_channels[block_id]\n",
    "#     if cross_attention_dim is None and (name.startswith(\"up_blocks\") ) :\n",
    "#         attn_procs[name] =  SpatialAttnProcessor2_0(id_length = id_length)\n",
    "#         total_count +=1\n",
    "#     else:\n",
    "#         attn_procs[name] = AttnProcessor()\n",
    "# print(\"successsfully load consistent self-attention\")\n",
    "# print(f\"number of the processor : {total_count}\")\n",
    "# unet.set_attn_processor(copy.deepcopy(attn_procs))\n",
    "# # print(\"unet\", unet)\n",
    "# global mask1024,mask4096\n",
    "# mask1024, mask4096 = cal_attn_mask_xl(total_length,id_length,sa32,sa64,height,width,device=device,dtype= torch.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get(\"DEBUG_MODE\") == \"true\":\n",
    "    print(\"mask1024\", mask1024.shape)\n",
    "    print(\"mask4096\", mask4096.shape)\n",
    "    print(mask1024)\n",
    "    print(mask1024.shape[0] // 5 * 4)\n",
    "    print(mask1024[196:].shape)\n",
    "    total_length = 5\n",
    "    id_length = 4\n",
    "    attention_mask = mask1024[:mask1024.shape[0] // total_length * id_length,:mask1024.shape[0] // total_length * id_length]\n",
    "    print(attention_mask.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_module_path = \"/userhome/37/ahhfdkx/AnimateDiff_sdxl/models/Motion_Module/mm_sdxl_v10_beta.ckpt\"\n",
    "motion_module_ckpt = torch.load(motion_module_path, map_location=\"cpu\")\n",
    "motion_module_state_dict = {}\n",
    "\n",
    "for k, v in motion_module_ckpt.items():\n",
    "    if 'motion_module' in k and k in pipe.unet.state_dict().keys():\n",
    "        motion_module_state_dict[k] = v\n",
    "        m_k = k\n",
    "    elif 'motion_module' in k and k not in pipe.unet.state_dict().keys():\n",
    "        print(k)\n",
    "pipe.unet.load_state_dict(motion_module_state_dict, strict=False)   \n",
    "del motion_module_ckpt\n",
    "del motion_module_state_dict\n",
    "print(f'Loading motion module from {motion_module_path}...')\n",
    "# print(unet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the text description for the comics\n",
    "Tips: Existing text2image diffusion models may not always generate images that accurately match text descriptions. Our training-free approach can improve the consistency of characters, but it does not enhance the control over the text. Therefore, in some cases, you may need to carefully craft your prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_scale = 5.0\n",
    "seed = 2047\n",
    "sa32 = 0.5\n",
    "sa64 = 0.5\n",
    "id_length = 4\n",
    "num_steps = 30\n",
    "general_prompt = \"a man with a black suit\"\n",
    "negative_prompt = \"naked, deformed, bad anatomy, disfigured, poorly drawn face, mutation, extra limb, ugly, disgusting, poorly drawn hands, missing limb, floating limbs, disconnected limbs, blurry, watermarks, oversaturated, distorted hands, amputation\"\n",
    "prompt_array = [\"wake up in the bed\",\n",
    "                \"have breakfast\",\n",
    "                \"is on the road, go to the company\",\n",
    "                \"work in the company\",\n",
    "                \"running in the playground\",\n",
    "                \"reading book in the home\"\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.cuda.set_device(0)  # 3 指的是第三块 GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from animatediff.pipelines.pipeline_animation import AnimationPipeline\n",
    "pipe = AnimationPipeline(\n",
    "    unet=unet, vae=pipe.vae, tokenizer=pipe.tokenizer, text_encoder=pipe.text_encoder, scheduler=pipe.scheduler,\n",
    "    text_encoder_2=pipe.text_encoder_2, tokenizer_2=pipe.tokenizer_2,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "id_prompts ['comic a man with a black suit,wake up in the bed . graphic illustration, comic art, graphic novel art, vibrant, highly detailed', 'comic a man with a black suit,have breakfast . graphic illustration, comic art, graphic novel art, vibrant, highly detailed', 'comic a man with a black suit,is on the road, go to the company . graphic illustration, comic art, graphic novel art, vibrant, highly detailed', 'comic a man with a black suit,work in the company . graphic illustration, comic art, graphic novel art, vibrant, highly detailed']\n",
      "comic a man with a black suit,wake up in the bed . graphic illustration, comic art, graphic novel art, vibrant, highly detailed\n",
      "photograph, deformed, glitch, noisy, realistic, stock photo, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry naked, deformed, bad anatomy, disfigured, poorly drawn face, mutation, extra limb, ugly, disgusting, poorly drawn hands, missing limb, floating limbs, disconnected limbs, blurry, watermarks, oversaturated, distorted hands, amputation\n"
     ]
    }
   ],
   "source": [
    "def apply_style_positive(style_name: str, positive: str):\n",
    "    p, n = styles.get(style_name, styles[DEFAULT_STYLE_NAME])\n",
    "    return p.replace(\"{prompt}\", positive) \n",
    "def apply_style(style_name: str, positives: list, negative: str = \"\"):\n",
    "    p, n = styles.get(style_name, styles[DEFAULT_STYLE_NAME])\n",
    "    return [p.replace(\"{prompt}\", positive) for positive in positives], n + ' ' + negative\n",
    "### Set the generated Style\n",
    "style_name = \"Comic book\"\n",
    "setup_seed(seed)\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "print(id_length)\n",
    "prompts = [general_prompt+\",\"+prompt for prompt in prompt_array]\n",
    "id_prompts = prompts[:id_length]\n",
    "torch.cuda.empty_cache()\n",
    "write = True\n",
    "cur_step = 0\n",
    "attn_count = 0\n",
    "id_prompts, negative_prompt = apply_style(style_name, id_prompts, negative_prompt)\n",
    "print(\"id_prompts\", id_prompts)\n",
    "print(id_prompts[0])\n",
    "print(negative_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.unet = pipe.unet.half()\n",
    "pipe.text_encoder = pipe.text_encoder.half()\n",
    "pipe.text_encoder_2 = pipe.text_encoder_2.half()\n",
    "pipe.enable_model_cpu_offload()\n",
    "pipe.enable_vae_slicing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_step 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:20<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample tensor([[[[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           ...,\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           ...,\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           ...,\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           ...,\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           ...,\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           ...,\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan]]],\n",
      "\n",
      "\n",
      "         [[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           ...,\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           ...,\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           ...,\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           ...,\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           ...,\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           ...,\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan]]],\n",
      "\n",
      "\n",
      "         [[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           ...,\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           ...,\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           ...,\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           ...,\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           ...,\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           ...,\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan]]]]], dtype=torch.float16)\n",
      "Saving video grid to ./output/sample_0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/37/ahhfdkx/StoryDiffusion/animatediff/utils/util.py:33: RuntimeWarning: invalid value encountered in cast\n",
      "  x = (x * 255).numpy().astype(np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_step 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m id_prompt \u001b[38;5;129;01min\u001b[39;00m id_prompts:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcur_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, cur_step)\n\u001b[0;32m---> 10\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                \u001b[49m\u001b[43mid_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m\t\t\t\t\u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# height is global once initialized all the sames\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m                \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[43msingle_model_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# video length = 8, 对应的attn mask 就会计算错误 ？？？\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m\t\t\t\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvideos\n\u001b[1;32m     20\u001b[0m     id_images\u001b[38;5;241m.\u001b[39mappend(sample)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m\"\u001b[39m, sample)\n",
      "File \u001b[0;32m~/miniconda3/envs/storydiffusion/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/StoryDiffusion/animatediff/pipelines/pipeline_animation.py:853\u001b[0m, in \u001b[0;36mAnimationPipeline.__call__\u001b[0;34m(self, prompt, prompt_2, single_model_length, height, width, num_inference_steps, denoising_end, guidance_scale, negative_prompt, negative_prompt_2, num_videos_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds, output_type, return_dict, callback, callback_steps, cross_attention_kwargs, guidance_rescale, original_size, crops_coords_top_left, target_size)\u001b[0m\n\u001b[1;32m    850\u001b[0m     noise_pred \u001b[38;5;241m=\u001b[39m rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale\u001b[38;5;241m=\u001b[39mguidance_rescale)\n\u001b[1;32m    852\u001b[0m \u001b[38;5;66;03m# compute the previous noisy sample x_t -> x_t-1\u001b[39;00m\n\u001b[0;32m--> 853\u001b[0m latents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_step_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# call the callback, if provided\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(timesteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m ((i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m num_warmup_steps \u001b[38;5;129;01mand\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39morder \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/storydiffusion/lib/python3.10/site-packages/diffusers/schedulers/scheduling_euler_discrete.py:480\u001b[0m, in \u001b[0;36mEulerDiscreteScheduler.step\u001b[0;34m(self, model_output, timestep, sample, s_churn, s_tmin, s_tmax, s_noise, generator, return_dict)\u001b[0m\n\u001b[1;32m    476\u001b[0m sample \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    478\u001b[0m sigma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmas[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_index]\n\u001b[0;32m--> 480\u001b[0m gamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(s_churn \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmas\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m s_tmin \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m sigma \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m s_tmax \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    482\u001b[0m noise \u001b[38;5;241m=\u001b[39m randn_tensor(\n\u001b[1;32m    483\u001b[0m     model_output\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mmodel_output\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mmodel_output\u001b[38;5;241m.\u001b[39mdevice, generator\u001b[38;5;241m=\u001b[39mgenerator\n\u001b[1;32m    484\u001b[0m )\n\u001b[1;32m    486\u001b[0m eps \u001b[38;5;241m=\u001b[39m noise \u001b[38;5;241m*\u001b[39m s_noise\n",
      "File \u001b[0;32m~/miniconda3/envs/storydiffusion/lib/python3.10/site-packages/torch/_tensor.py:904\u001b[0m, in \u001b[0;36mTensor.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;21m__neg__\u001b[39m \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39m_TensorBase\u001b[38;5;241m.\u001b[39mneg\n\u001b[1;32m    902\u001b[0m \u001b[38;5;21m__abs__\u001b[39m \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39m_TensorBase\u001b[38;5;241m.\u001b[39mabs\n\u001b[0;32m--> 904\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    906\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__len__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"DEBUG_MODE\"] = \"false\"\n",
    "from animatediff.utils.util import save_videos_grid\n",
    "id_prompts = [\"A panda standing on a surfboard in the ocean in sunset, 4k, high resolution.Realistic, Cinematic, high resolution\",\n",
    "              \"A disoriented astronaut, lost in a galaxy of swirling colors, floating in zero gravity, grasping at memories, poignant loneliness, stunning realism, cosmic chaos, emotional depth, 12K, hyperrealism, unforgettable, mixed media, celestial, dark, introspective\"\n",
    "]\n",
    "id_images = []\n",
    "for id_prompt in id_prompts:\n",
    "    print(\"cur_step\", cur_step)\n",
    "    sample = pipe(\n",
    "                id_prompt,\n",
    "\t\t\t\tnum_inference_steps = num_steps, \n",
    "                guidance_scale=guidance_scale, \n",
    "                height = height, # height is global once initialized all the sames\n",
    "                width = width,\n",
    "                negative_prompt = negative_prompt,\n",
    "                single_model_length = 8, # video length = 8, 对应的attn mask 就会计算错误 ？？？\n",
    "                generator = generator\n",
    "\t\t\t).videos\n",
    "    id_images.append(sample)\n",
    "    print(\"sample\", sample)\n",
    "    # __import__('ipdb').set_trace()\n",
    "    save_videos_grid(sample, f\"./output/sample_{cur_step}.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continued Creation\n",
    "From now on, you can create endless stories about this character without worrying about memory constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make pictures into comics"
   ]
  }
 ],
 "metadata": {
  "fileId": "51613593-0d85-430e-8fce-c85e580fc483",
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
